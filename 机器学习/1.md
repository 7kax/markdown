# Probability Distributions

## Probability Theory

- Denotation of **joint probability**: $P(X=x_i,Y=y_i)$
- Denotation of **conditional probability**: $P(X=x_i|Y=y_i)$
- Denotation of **marginal probability**:
  $$
  P(X=x_i) = \sum_{j} P(X=x_i, Y=y_j)
  $$
- **Bayes' theorem**:
  $$
  P(X=x_i|Y=y_i) = \frac{P(Y=y_i|X=x_i)P(X=x_i)}{P(Y=y_i)}
  $$
- **Expectation**:
  $$
  \mathbb{E}[f] = \sum_x P(x)f(x)
  $$
  $$
  \mathbb{E}[f] = \int P(x)f(x)dx
  $$
  where $f$ is a function of $x$ and $P(x)$ is the probability of $x$.
- **Variance**:
  $$
  var[f] = \mathbb{E}[(f(x) - \mathbb{E}[f(x)])^2]
  $$
- **Covariance**:
  $$
  cov[f, g] = \mathbb{E}[(f(x) - \mathbb{E}[f(x)])(g(x) - \mathbb{E}[g(x)])]
  $$

## Bernoulli Distribution

Given a binary variable $x \in \{0, 1\}$, the probability can be written as:

$$
\begin{cases}
p(x=1|\mu) = \mu \\
p(x=0|\mu) = 1 - \mu
\end{cases}
$$

where $\mu \in [0, 1]$.

The **Bernoulli** distribution:

$$
Bern(x|\mu) = \mu^x (1 - \mu)^{1-x}
$$

Its mean and variance are:

$$
\begin{cases}
\mathbb{E}[x] = \mu \\
var[x] = \mu(1-\mu)
\end{cases}
$$

Suppose we have a data set $\mathcal{D} = \{x_1, \dots, x_N\}$, on the assumption that the data points are **independent and identically distributed**(**i.i.d.**), the **likelihood function** is:

$$
p(\mathcal{D}|\mu) = \prod_{n=1}^N p(x_n|\mu) = \prod_{n=1}^N \mu^{x_n} (1-\mu)^{1-x_n}
$$

The log likelihood function is:

$$
\ln p(\mathcal{D}|\mu) = \sum_{n=1}^N \{x_n \ln \mu + (1-x_n) \ln (1-\mu)\}
$$

To find the **maximum** likelihood solution, we set the derivative of the log likelihood function to zero:

$$
\frac{d}{d\mu} \ln p(\mathcal{D}|\mu) = \sum_{n=1}^N \frac{x_n}{\mu} - \frac{1-x_n}{1-\mu} = 0
$$

The solution is:

$$
\mu_{ML} = \frac{1}{N} \sum_{n=1}^N x_n
$$

## Binomial Distribution

$$
Bin(m|N, \mu) = \binom{N}{m} \mu^m (1-\mu)^{N-m}
$$

The mean and variance are:

$$
\begin{cases}
\mathbb{E}[m] = N\mu \\
var[m] = N\mu(1-\mu)
\end{cases}
$$

## Beta Distribution

$$
Beta(\mu|a, b) = \frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)} \mu^{a-1} (1-\mu)^{b-1}
$$

where $\Gamma(x)$ is the **Gamma** function: $\Gamma(x) = \int_0^\infty u^{x-1} e^{-u} du$, and $\Gamma(n) = (n-1)!$ for positive integer $n$.

The coefficients $a$ and $b$ ensure that the distribution is normalized: $\int_0^1 Beta(\mu|a, b) d\mu = 1$, and are called **hyperparameters**.

The mean and variance are:

$$
\begin{cases}
\mathbb{E}[\mu] = \frac{a}{a+b} \\
var[\mu] = \frac{ab}{(a+b)^2(a+b+1)}
\end{cases}
$$

Then the posterior distribution is:

$$
p(\mu|m, l, a, b) = \frac{\Gamma(m+a+l+b)}{\Gamma(m+a)\Gamma(l+b)} \mu^{m+a-1} (1-\mu)^{l+b-1}
$$

where $m$ is the number of heads, $l$ is the number of tails.

### Illustration

The prior is given by a beta distribution with $a = b = 2$, the likelihood function is given by a binomial distribution with $N=m=1$, then the posterior distribution is given by a beta distribution with $a = 3$, $b = 2$.

![](./images/1.png)

### Prediction

Given a data set $\mathcal{D} = \{x_1, \dots, x_N\}$, the predictive distribution is:

$$
p(x=1|\mathcal{D}) = \int_0^1 p(x=1|\mu) p(\mu|\mathcal{D}) d\mu = \int_0^1 \mu p(\mu|\mathcal{D}) d\mu = \mathbb{E}[\mu|\mathcal{D}]
$$

where $p(\mu|\mathcal{D})$ is the posterior distribution.

Together with the fact that $\mathbb{E}[\mu] = \frac{a}{a+b}$, we have:

$$
p(x=1|\mathcal{D}) = \frac{m+a}{m+a+l+b}
$$

## Multinomial Variables

Variables $x$ is now a $K$-dimensional vector $\mathbf{x} = (x_1, \dots, x_K)^T$, where $x_k \in \{0, 1\}$ and $\sum_{k=1}^K x_k = 1$.

Denote the probability of $x_k = 1$ by $\mu_k$, then the probability of $\mathbf{x}$ is:

$$
p(\mathbf{x}|\mu) = \prod_{k=1}^K \mu_k^{x_k}
$$

where $\mu = (\mu_1, \dots, \mu_K)^T$ and $\sum_{k=1}^K \mu_k = 1$, and $\mathbb{E}[\mathbf{x}] = \mu$.

Consider a data set $\mathcal{D} = \{\mathbf{x}_1, \dots, \mathbf{x}_N\}$, the likelihood function is:

$$
p(\mathcal{D}|\mu) = \prod_{n=1}^N \prod_{k=1}^K \mu_k^{x_{nk}} = \prod_{k=1}^K \mu_k^{(\sum_{n=1}^N x_{nk})} = \prod_{k=1}^K \mu_k^{m_k}
$$

where $m_k = \sum_{n=1}^N x_{nk}$.

We can use **Lagrange multipliers** to maximize the likelihood function under the constraint $\sum_{k=1}^K \mu_k = 1$:

$$
\ln p(\mathcal{D}|\mu) + \lambda (\sum_{k=1}^K \mu_k - 1)
$$

The solution is:

$$
\mu_k^{ML} = \frac{m_k}{N}
$$

Consider the joint distribution of $m_1, \dots, m_K$:

$$
p(m_1, \dots, m_K|N, \mu) = \binom{N}{m_1, \dots, m_K} \prod_{k=1}^K \mu_k^{m_k}
$$

where $\binom{N}{m_1, \dots, m_K} = \frac{N!}{m_1! \dots m_K!}$ and $\sum_{k=1}^K m_k = N$.

## The Dirichlet Distribution

$$
Dir(\mu|\alpha) = \frac{\Gamma(\alpha_0)}{\Gamma(\alpha_1) \dots \Gamma(\alpha_K)} \prod_{k=1}^K \mu_k^{\alpha_k - 1}
$$

where $\alpha = (\alpha_1, \dots, \alpha_K)^T$, $\alpha_0 = \sum_{k=1}^K \alpha_k$, and $\sum_{k=1}^K \mu_k = 1$.

The posterior distribution is:

$$
\begin{aligned}
p(\mu|\mathcal{D}, \alpha) &= Dir(\mu|\alpha + m) \\
&= \frac{\Gamma(\alpha_0 + N)}{\Gamma(\alpha_1 + m_1) \dots \Gamma(\alpha_K + m_K)} \prod_{k=1}^K \mu_k^{\alpha_k + m_k - 1}
\end{aligned}
$$

## Gaussian Distribution

$$
\mathcal{N}(x|\mu, \sigma^2) = \frac{1}{(2\pi\sigma^2)^{1/2}} \exp \{-\frac{1}{2\sigma^2} (x-\mu)^2\}
$$

where $\mu$ is the mean and $\sigma^2$ is the variance.

For a $D$-dimensional vector $\mathbf{x}$, the Gaussian distribution is:

$$
\mathcal{N}(\mathbf{x}|\mathbf{\mu}, \mathbf{\Sigma}) = \frac{1}{(2\pi)^{D/2}} \frac{1}{|\mathbf{\Sigma}|^{1/2}} \exp \{-\frac{1}{2} (\mathbf{x}-\mathbf{\mu})^T \mathbf{\Sigma}^{-1} (\mathbf{x}-\mathbf{\mu})\}
$$

where $\mathbf{\mu}$ is the mean vector, $\mathbf{\Sigma}$ is the covariance matrix, and $|\mathbf{\Sigma}|$ is the determinant of $\mathbf{\Sigma}$.

### Mahalanobis Distance

$$
\Delta^2(\mathbf{x}, \mathbf{\mu}) = (\mathbf{x}-\mathbf{\mu})^T \mathbf{\Sigma}^{-1} (\mathbf{x}-\mathbf{\mu})
$$

$\Delta$ is called the **Mahalanobis distance** from $\mathbf{x}$ to $\mathbf{\mu}$ and reduces to the **Euclidean distance** when $\mathbf{\Sigma} = \mathbf{I}$.

Consider the eigenvector equation for the covariance matrix:

$$
\mathbf{\Sigma} \mathbf{u}_i = \lambda_i \mathbf{u}_i
$$

where $\lambda_i$ is the $i_{th}$ eigenvalue and $\mathbf{u}_i$ is the corresponding eigenvector.

Since $\mathbf{\Sigma}$ is real and symmetric, the eigenvalues are real and the eigenvectors are orthogonal, so we can write:

$$
\mathbf{\mu}_i^T \mathbf{\mu}_j = \mathbf{I}_{ij}
$$

The covariance matrix can be written as:

$$
\mathbf{\Sigma} = \sum_{i=1}^d \lambda_i \mathbf{\mu}_i \mathbf{\mu}_j^T
$$

The inverse of the covariance matrix is:

$$
\mathbf{\Sigma}^{-1} = \sum_{i=1}^d \frac{1}{\lambda_i} \mathbf{\mu}_i \mathbf{\mu}_j^T
$$

And the Mahalanobis distance is:

$$
\Delta^2(\mathbf{x}, \mathbf{\mu}) = (\mathbf{x}-\mathbf{\mu})^T \mathbf{\Sigma}^{-1} (\mathbf{x}-\mathbf{\mu}) = \sum_{i=1}^d \frac{y_i^2}{\lambda_i}
$$

where $y_i = \mathbf{\mu}_i^T (\mathbf{x}-\mathbf{\mu})$. The Mahalanobis distance is the Euclidean distance in the transformed space.

# Linear Regression

## Polynomial Curve Fitting

$$
y(x, \mathbf{w}) = w*0 + w_1 x + w_2 x^2 + \dots + w_M x^M = \sum_{j=0}^M w_j x^j
$$

The polynomial function is a **nonlinear** function of the input variable $x$, but it is a **linear** function of the coefficients $\mathbf{w}$.

### Error Function

The sum of squares error(**SSE**) function is:

$$
E(\mathbf{w}) = \frac{1}{2} \sum_{n=1}^N \{y(x_n, \mathbf{w}) - t_n\}^2
$$

where $t_n$ is the target value for the $n_{th}$ input $x_n$.

The minimization of the error function has a unique solution denoted by $\mathbf{w}^*$

Root mean square error(**RMS**):

$$
E_{RMS} = \sqrt{2E(\mathbf{w}^*)/N}
$$

### Model Selection

The model selection problem is to choose $M$.

### Regularization

Regularization is a technique to control the complexity of the model and avoid **overfitting**.

$$
\tilde{E}(\mathbf{w}) = \frac{1}{2} \sum_{n=1}^N \{y(x_n, \mathbf{w}) - t_n\}^2 + \frac{\lambda}{2} \|\mathbf{w}\|^2
$$

where $\lambda$ is a positive constant.

## Probability Perspective for Regression
