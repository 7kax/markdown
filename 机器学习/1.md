# 概率论部分结论

## 贝叶斯公式

设$A,B$是两个事件，且$P(B) > 0$，则有

$$
P(A|B) = \frac{P(B|A)P(A)}{P(B)}
$$

## 协方差

设$x,y$是两个随机变量，其数学期望分别为$\mu_x,\mu_y$，则称

$$
Cov(x,y) = E_{x,y}[(x-\mu_x)(y-\mu_y)]=E_{x,y}[xy]-\mu_x\mu_y
$$

为$x,y$的协方差.

如果$\boldsymbol{x}=(x_1,x_2,\cdots,x_n)^T,\boldsymbol{y}=(y_1,y_2,\cdots,y_n)^T$是两个随机向量，其数学期望分别为$\boldsymbol{\mu_x},\boldsymbol{\mu_y}$，则称

$$
Cov(\boldsymbol{x},\boldsymbol{y}) = E_{\boldsymbol{x},\boldsymbol{y}}[(\boldsymbol{x}-\boldsymbol{\mu_x})(\boldsymbol{y}-\boldsymbol{\mu_y})^T]=E_{\boldsymbol{x},\boldsymbol{y}}[\boldsymbol{xy}^T]-\boldsymbol{\mu_x}\boldsymbol{\mu_y}^T
$$

为$\boldsymbol{x},\boldsymbol{y}$的协方差矩阵

如果考虑向量$\boldsymbol{x}$各个分量之间的协方差，可以简记

$$
Cov(\boldsymbol{x}) = Cov(\boldsymbol{x},\boldsymbol{x})
$$

## 贝叶斯概率

先验概率为$p(w)$, 观测数据为$\mathcal{D}= \{t_1,t_2,\cdots,t_N\}$，后验概率为$p(w|\mathcal{D})$，则有

$$
p(w|\mathcal{D}) = \frac{p(\mathcal{D}|w)p(w)}{p(\mathcal{D})}
$$

其中$p(\mathcal{D}|w)$称为**似然函数(likelihood function)**

## 高维正态分布

设$\boldsymbol{x}$是一个$n$维随机向量，高斯分布定义为

$$
\mathcal{N}(\boldsymbol{x}|\boldsymbol{\mu},\boldsymbol{\Sigma})= \frac{1}{(2\pi)^{n/2}|\boldsymbol{\Sigma}|^{1/2}}\exp\left\{-\frac{1}{2}(\boldsymbol{x}-\boldsymbol{\mu})^T\boldsymbol{\Sigma}^{-1}(\boldsymbol{x}-\boldsymbol{\mu})\right\}
$$

其中$\boldsymbol{\mu}$是均值向量，$\boldsymbol{\Sigma}$是协方差矩阵

假设已知上述分布, 又得到数据集$\bold{x}=(x_1,x_2,\cdots,x_N)^T$, 得到该数据集的概率为

$$
p(\bold{x}|\boldsymbol{\mu},\boldsymbol{\Sigma}) = \prod_{n=1}^N\mathcal{N}(x_n|\boldsymbol{\mu},\boldsymbol{\Sigma})
$$

根据定义, 这是高斯分布的**似然函数**

## 高斯噪声

对于输入$x$, 需要预测$t$, 我们假设$t$服从高斯分布, 均值为$y(x,\boldsymbol{w})$, 方差为$\beta^{-1}$, 则有

$$
p(t|x,\boldsymbol{w},\beta) = \mathcal{N}(t|y(x,\boldsymbol{w}),\beta^{-1})
$$

基于训练数据, 通过最大似然法, 可以得到$\boldsymbol{w}$的估计值$\boldsymbol{w}_{ML}$, 以及$\beta$的估计值$\beta_{ML}$, 用来计算预测值

进一步引入多项式系数$\boldsymbol{w}$的先验分布, 表示为

$$
p(\boldsymbol{w}|\alpha) = \mathcal{N}(\boldsymbol{w}|\boldsymbol{0},\alpha^{-1}\boldsymbol{I})=\left(\frac{\alpha}{2\pi}\right)^{(M+1)/2}\exp\left\{-\frac{\alpha}{2}\boldsymbol{w}^T\boldsymbol{w}\right\}
$$

其中$\alpha$是精度参数, $\boldsymbol{I}$是单位矩阵, $M$是多项式的阶数

则$\boldsymbol{w}$的后验分布正比于似然函数与先验分布的乘积, 即

$$
p(\boldsymbol{w}|\boldsymbol{x},\boldsymbol{t},\alpha,\beta) \propto p(\boldsymbol{t}|\boldsymbol{x},\boldsymbol{w},\beta)p(\boldsymbol{w}|\alpha)
$$

## 贝叶斯曲线拟合

给定训练数据$\bold{x}$和$\bold{t}$, 以及一个新的测试点$x$, 预测值$t$的分布$p(t|x,\bold{x},\bold{t})$, 这里假设参数$\alpha,\beta$已知, 则有

$$
p(t|x,\bold{x},\bold{t}) = \int p(t|x,\boldsymbol{w})p(\boldsymbol{w}|\bold{x},\bold{t})d\boldsymbol{w}
$$

注意: 项$p(\boldsymbol{w}|\bold{x},\bold{t})$隐含了对$\alpha,\beta$的依赖, 项$p(t|x,\boldsymbol{w})$隐含了对$\beta$的依赖

# 决策论

假设我们有输入$\boldsymbol{x}$, 可能属于类型$\mathcal{C}_k$, 但是我们不知道$\boldsymbol{x}$的类型, 我们的任务是根据$\boldsymbol{x}$的特征, 选择一个最优的类型$\mathcal{C}_k$

## 最小错误/最大正确分类率

我们的目标是最大化下式

$$
p(\text{correct}) = \sum_{k=1}^Kp(\boldsymbol{x}\in\mathcal{R}_k,\mathcal{C}_k) = \sum_{k=1}^K\int_{\mathcal{R}_k}p(\boldsymbol{x},\mathcal{C}_k)d\boldsymbol{x}
$$

## 最小化期望损失

假设我们有输入$\boldsymbol{x}$属于类型$\mathcal{C}_k$, 但是我们将其分类为$\mathcal{C}_j$, 则损失为$L_{kj}$, 即**损失矩阵**的一个元素, 我们的目标是最小化平均损失, 即

$$
\mathbb{E}[L] = \sum_{k=1}^K\sum_{j=1}^KL_{kj}p(\boldsymbol{x}\in\mathcal{R}_j,\mathcal{C}_k)=\sum_{k=1}^K\sum_{j=1}^KL_{kj}\int_{\mathcal{R}_j}p(\boldsymbol{x},\mathcal{C}_k)d\boldsymbol{x}
$$

我们的目标是选择区域$\mathcal{R}_j$, 使得$\mathbb{E}[L]$最小, 对于每个$\boldsymbol{x}$, 我们需要最小化$\sum_kL_{kj}p(\boldsymbol{x},\mathcal{C}_k)$, 又因为$p(\boldsymbol{x},\mathcal{C}_k)=p(\mathcal{C}_k|\boldsymbol{x})p(\boldsymbol{x})$, 所以我们需要最小化的就是

$$
\sum_kL_{kj}p(\mathcal{C}_k|\boldsymbol{x})
$$

我们选择合适的$j$, 使得上式最小

## 拒绝选项

引入阈值$\theta$, 如果$\max_kp(\mathcal{C}_k|\boldsymbol{x}) < \theta$, 则拒绝分类, 否则选择$\max_kp(\mathcal{C}_k|\boldsymbol{x})$对应的类型
